
\documentclass[a4paper,german]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{enumerate}


%This is how you can create a "claim"-environment (or a lemma/Theorem/definition etc environment)
\newtheorem{claim}{Claim}

%This is how you can create a command of your own, e.g. to simplify the usage signs you often use.
\newcommand{\E}{\mathbb{E}}

%Titlepage:
\title{Randomized Algorithms and Probabilistic Methods: Graded Homework 3}
\author{ Kevin Klein, collaborators: Ramon Braunwarth}
\date{December 8 2017}


\begin{document}
\maketitle

\section*{Exercise 1}
\subsection*{a)}
For \(k \in \{1, \dots, n\}\), we know that there are \( n \choose k\) subgraphs of \(G\) with \(k\) vertices. For one of those subgraphs to be fully connected, i.e. \(K_k\), all of the \(k \choose 2\) possible edges have to be part of the edge set of \(G\).  Defining \(X_k\) to be the number of cliques of size \(k\) in \(G\), its expectation equals  \( {n \choose k} (1/2)^{k \choose 2}\), as every edge is selected with probability \(1/2\). 

We observe that \(\Pr [X \geq k] = \Pr[X_k > 0]\) because if a clique of size \(k + l, l \geq 1\) exists in \(G\), \(G\) also has cliques of size \(k\). As \(X_k\) can only take on positive  integer values, \(\E[X_k]\) has to be greater or equal \(Pr[X \geq k]\). For some \(c \in \mathbb{N}_{>0}\), independent of \(n\):

\begin{align*}
\E[X] &= \sum_{k=1}^\infty \Pr[X \geq k] =  \sum_{k=1}^n \Pr[X \geq k] \\
&= \sum_{k=1}^{c \cdot \lceil log(n) \rceil} \Pr[X \geq k]  + \sum_{i = c \cdot \lceil log(n) \rceil + 1} ^ n \Pr[X \geq k]   \\
&\leq \sum_{k=1}^{c \cdot \lceil log(n) \rceil} 1  + \sum_{i = c \cdot \lceil log(n) \rceil + 1} ^ n \Pr[X \geq k]   \\
&\leq  c \cdot \lceil log(n) \rceil + \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n \E[X_k] \\
&= c \cdot \lceil log(n) \rceil+ \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n {n \choose k} (1/2)^{k \choose 2}
\end{align*}

It is left to show that \( \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n {n \choose k} (1/2)^{k \choose 2}  \) is in \(\mathcal{O}(log(n))\). \( {n \choose k} \) can be easily bounded by \(n^k\) from above. Also, we observe the following:
\begin{align*}
k \geq c \cdot \lceil log(n) \rceil + 1 &\Rightarrow {k \choose 2} = \frac{k(k-1)}{2} \geq \frac{k\cdot c \cdot log(n)}{2} \\
& \Rightarrow (1/2)^{k \choose 2} \leq (1/2) ^ \frac{k\cdot c \cdot log(n)}{2} = n ^ {-{k \cdot c /2}}
\end{align*}
Combining those two bounds we obtain:
\begin{align*}
\sum_{k=c \cdot \lceil log(n) \rceil + 1}^n {n \choose k} (1/2)^{k \choose 2} &\leq \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n n^k n ^ {-{k \cdot c /2}} = \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n  n^{k(1 - c/2)} \\
&\leq \sum_{k=4 \cdot \lceil log(n) \rceil + 1}^n n ^ {-k} \text{ for } c = 4\\\
&\leq \sum_{i=1}^n n^{-k} = H_n \leq log(n + 1) \leq 2 \cdot log(n) \text{ for } n \geq 3
\end{align*}
We conclude that \(\E[X] \leq  4 \cdot log(n) + 2 \cdot log(n)\). Equivalently, for \(c' = 6\) we have \(\E[X] \leq c' log(n)\), which is what we intended to show. For \(n \in \{1,2\} \) we can fix the constant to be a very large number. 

\subsection*{b)}
Assuming \(\E[X] = \Theta(log(n))\) we can express the desired probability's negation in a convenient fashion.
\begin{align*}
&\Pr[X \notin  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]] \\
&= \Pr[X < (1 - \alpha(n)) \E[X]\ \wedge  X > (1 + \alpha(n)) \E[X]] \\
&= \Pr[X - \E[X] < - \alpha(n) \E[X]\ \wedge X - \E[X] > \alpha(n) \E[X]] \\
&= \Pr[|X - \E[X]| > \alpha(n)\E[X]] \\
& \leq \Pr[|X - \E[X]| \geq \alpha(n)\E[X]]
\end{align*}
We recognize a form from Lemma 8.2. In order to make use of it, we need to ensure its conditions are satisfied. Let \( \Omega = \Omega_1 \times \dots \times \Omega_N\) with \(N = n\) and each \(\Omega_i\) describing whether all of the \(n-1\) edges of vertex \(i\) are included or not. It follows directly that the effect of each dimension \(c_i\) is at most 1. Let \(\omega \in \Omega\) be a selection of such vertices and \(X(\omega)\) its maximum clique size. Setting \(r = X(\omega)\) trivially fulfills \(r \leq X(\omega)\). Let us define \(J\) to be the subset of \(\{1, \dots, n\}\) that indicates which vertices compose a \(X(\omega)\)-clique - arbitrarily in case of ties. Then, for each \(\omega' \in \Omega\) with \(\forall i \in J, w'_i = w_i\), the size of the maximum clique in \(\omega'\) can only increase, i.e. \(X(\omega') \geq X(\omega) = r\). Moreover:
\begin{align*}
 \sum_{i \in J} c_i^2 \leq \sum_{i \in J} 1 = |J| = X(\omega) = r
 \end{align*}
Thereby the necessary conditions hold true and Lemma 8.2.'s application yields:
\begin{align*}
&\Pr[X \notin  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]] \\
&\leq 4 e^{- \Omega(\frac{(\alpha(n)\E[X])^2}{\E[X] + \alpha(n) \E[X]})} =  4 e^{- \Omega(\frac{\alpha(n)^2\E[X]}{1 + \alpha(n)})}\\
\end{align*}
We define \(\alpha(n) = log(n) ^ {-1/4} \in o(1)\). 
\begin{align*}
\lim_{n \to \infty} \frac{\alpha(n)^2 log(n)}{1 + \alpha(n)} = \lim_{n \to \infty} \frac{log(n)}{\sqrt{log(n)} \cdot 1} = \lim_{n \to \infty} \sqrt{log(n)} \to \infty\\
\end{align*}
It follows directly that \(\Pr[X \notin  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]] \) tends to 0 for large enough \(n\).
\begin{align*}
&\Pr[X \in  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]]  \\
&= 1 - \Pr[X \notin  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]] \\
& \to 1
\end{align*}

\section*{Exercise 2}
We define \((Y_t)_{t \in \mathbb{N}}\) to be the amount of 'missing mutants', i.e. \(n - |X_t|\). Hence we are looking to bind \(\E[T]\) for \(Y_t\) from above. We define \(u_t\) and \(v_t\) to be the \(u\) and \(v\) of the \(t\)-th round of the evolutionary process. As \(G = K_n\) and \(V_o = n/2\), we know that \(\Pr[u_t \in V_o] = \Pr[u_t \notin V_0] = 1/2\). \\
The process' drawing process weighs vertices from \(V_t\) with \(r\). Given that \(Y_t = y\), there are \( |X_t| = n - y\) such vertices. Hence:
\begin{align*}
	\Pr[v_t \in X_t] = \frac{r(n-y)}{r(n-y) + 1 \cdot y} \text{ and } \Pr[v \notin X_t] = \frac{1 \cdot y}{r(n-y) + 1 \cdot y} 
\end{align*}
As \(u_t\) is drawn after \(v_t\), there are only \(n-1\) vertices to choose from. In particular, we observe that \(y\) is at most \(n/2\) because \(X_t\) will always include \(V_0\). Hence the size of \(X_t \setminus V_0\) is given by \(|X_t| - |V_0| = n - y - n/2 = n/2 - y\).
\begin{align*}
	\Pr[u_t \in X_t \setminus V_0] = \frac{n/2 - y}{n-1} \text{ and } \Pr[u_t \notin X_t] = \frac{y}{n-1}
\end{align*}
Moreover, in order for \(Y_{t+1}\) to decrease over \(Y_t\), \(v_t\) has to be part of \(X_t\) and \(u_t\) be a node not yet contained in \(X_t\). Hence:
\begin{align*}
	\Pr[Y_{t+1} = Y_t - 1] &= \Pr[v_t \in X_t \wedge u_t \notin X_t] \\
	&= \Pr[v_t \in X_t] \Pr[u_t \notin X_t | v_t \in X_t] \\
	&= \frac{r(n-y)}{r(n-y) + y} \frac{y}{n-1}
\end{align*}
On the other hand, for \(Y_{t+1}\) to increase over \(Y_t\),  \(v_t\) has to be excluded from \(X_t\) and \(u_t\) be already contained in \(X_t\). Hence:
\begin{align*}
	\Pr[Y_{t+1} = Y_t + 1] &= \Pr[v_t \notin X_t \wedge u_t \in X_t \setminus V_0] \\
	&= \Pr[v_t \notin X_t] \Pr[u \in X_t \setminus V_0 | v_t \notin X_t] \\
	&= \frac{y}{r(n-y) + y} \frac{n/2 - y}{n-1}
\end{align*}
We will now inspect the drift in order to find a fitting function \(g\) to apply Theorem 9.11. We observe that \(Y_t\) can only remain, increase by one or decrease by one.
\begin{align*}
	\E[Y_{t+1} | Y_t = y] &= y \Pr[Y_{t+1} = Y_t] + (y-1)\Pr[Y_{t+1} = Y_t - 1] + (y+1)\Pr[Y_{t+1} = Y_t + 1] \\
	&=y - \Pr[Y_{t+1} = Y_t - 1] + \Pr[Y_{t+1} = Y_t + 1] \\
	&=y - (\Pr[Y_{t+1} = Y_t - 1] - \Pr[Y_{t+1} = Y_t + 1]) \\
	&= y -\frac{y}{(r(n-y) +y) (n-1)} (r(n-y) - (n/2 - y)) \\
	&=  y - \frac{y (y(1-r) +n (r-1) + n/2)}{(r(n-y) +y) (n-1)} \\
	&=  y - \frac{y ((r-1)(n-y) + n/2)}{(r(n-y) +y) (n-1)} \\
	&\leq y - \frac{y ((r-1)n/2 + n/2)}{(rn +y) (n-1)} \text{ with } (n-y \geq n/2, n-y \leq n) \\
	&= y - \frac{y \cdot r \cdot n/2}{(rn +y) (n-1)}\\
	&\leq y - \frac{y \cdot r}{2(rn +y)} \text { with } (n-1 \leq n)\\
	&\leq y - \frac{y \cdot r}{2(r+1)n} \text { with } (y \leq n) \\
	&\leq \frac{c \cdot y}{n} \text { with } (c \geq \frac{r}{2(r+1)})
\end{align*}

We observe that \( \int_1^{n/2} \frac{c \cdot y}{n} dy =  \left[ c\cdot y log(n)\right]_1^{n/2} = c\cdot n log(n)\) and \(\frac{1}{\frac{c \cdot 1}{n}} = \frac{n}{c}\). Hence, we can apply Theorem 9.12 with \(Y_0 = n/2, h(y) = \frac{c \cdot y}{n}\):
\begin{align*}
	\E[T] \leq \frac{c}{n} + c\cdot n log(n)
\end{align*} 
In conclusion, we have shown that for \(Y_t\) \(E[T] \in \mathcal{O}(nlog(n))\). According to the definition of \(Y_t\), it \(X_t = V\) if \(Y_t = 0\). Hence the bound on the process end also holds for \(X_t\).
\section*{Exercise 3}
\section*{Exercise 4}
\subsection*{(a)}
\subsection*{(b)}
\subsection*{(c)}
\begin{enumerate}[i]
	\item
	The definition of the tree makes it easy to see that each bin has to have a leaf to be associated with it, namely be connected to a ball with label 6. Moreover this ball will not be connected to another bin in the graph, by definition. (TODO(kkleindev): check). The presence of the ball of height \(h\), connected to bin \(u\), implies the presence of a leaf associated with \(u\) as well as the leafs present in the subtrees rooted in other balls connected to \(u\). We know that there are balls labeled from 7 to \(h -1\) connected to \(u\). Hence: \(t(h) = 1 + \sum_{i=7}^{h-1} t(i) \).
	\begin{claim} 
		\(\forall h \in \mathbb{N}, h \geq 7: t(h) = 2^{h-7}\)
	\end{claim}
	\begin{proof} 
		We observe that for \(h = 7\), there is one leaf, i.e. \(t(7) = 1 = 2 ^{7-7}.\) We will assume our hypothesis for some h and will show it holds for \(h+1\) as well.
		\begin{align*}
		t(h+1) = 1 + \sum \\
		\end{align*}
	\end{proof}
\item 
\end{enumerate}
\subsection*{(d)}
\end{document}
