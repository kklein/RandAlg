
\documentclass[a4paper,german]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{enumerate}


%This is how you can create a "claim"-environment (or a lemma/Theorem/definition etc environment)
\newtheorem{claim}{Claim}

%This is how you can create a command of your own, e.g. to simplify the usage signs you often use.
\newcommand{\E}{\mathbb{E}}

%Titlepage:
\title{Randomized Algorithms and Probabilistic Methods: Graded Homework 3}
\author{ Kevin Klein, collaborators: Ramon Braunwarth}
\date{December 8 2017}


\begin{document}
\maketitle

\section*{Exercise 1}
\subsection*{a)}
For \(k \in \{1, \dots, n\}\), we know that there are \( n \choose k\) subgraphs of \(G\) with \(k\) vertices. For one of those subgraphs to be fully connected, i.e. \(K_k\), all of the \(k \choose 2\) possible edges have to be part of the edge set of \(G\).  Defining \(X_k\) to be the number of cliques of size \(k\) in \(G\), its expectation equals  \( {n \choose k} (1/2)^{k \choose 2}\), as every edge is selected with probability \(1/2\). 

We observe that \(\Pr [X \geq k] = \Pr[X_k > 0]\) because if a clique of size \(k + l, l \geq 1\) exists in \(G\), \(G\) also has cliques of size \(k\). As \(X_k\) can only take on positive  integer values, \(\E[X_k]\) has to be greater or equal \(Pr[X \geq k]\). For some \(c \in \mathbb{N}_{>0}\), independent of \(n\):

\begin{align*}
\E[X] &= \sum_{k=1}^\infty \Pr[X \geq k] =  \sum_{k=1}^n \Pr[X \geq k] \\
&= \sum_{k=1}^{c \cdot \lceil log(n) \rceil} \Pr[X \geq k]  + \sum_{i = c \cdot \lceil log(n) \rceil + 1} ^ n \Pr[X \geq k]   \\
&\leq \sum_{k=1}^{c \cdot \lceil log(n) \rceil} 1  + \sum_{i = c \cdot \lceil log(n) \rceil + 1} ^ n \Pr[X \geq k]   \\
&\leq  c \cdot \lceil log(n) \rceil + \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n \E[X_k] \\
&= c \cdot \lceil log(n) \rceil+ \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n {n \choose k} (1/2)^{k \choose 2}
\end{align*}

It is left to show that \( \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n {n \choose k} (1/2)^{k \choose 2}  \) is in \(\mathcal{O}(log(n))\). \( {n \choose k} \) can be easily bounded by \(n^k\) from above. Also, we observe the following:
\begin{align*}
k \geq c \cdot \lceil log(n) \rceil + 1 &\Rightarrow {k \choose 2} = \frac{k(k-1)}{2} \geq \frac{k\cdot c \cdot log(n)}{2} \\
& \Rightarrow (1/2)^{k \choose 2} \leq (1/2) ^ \frac{k\cdot c \cdot log(n)}{2} = n ^ {-{k \cdot c /2}}
\end{align*}
Combining those two bounds we obtain:
\begin{align*}
\sum_{k=c \cdot \lceil log(n) \rceil + 1}^n {n \choose k} (1/2)^{k \choose 2} &\leq \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n n^k n ^ {-{k \cdot c /2}} = \sum_{k=c \cdot \lceil log(n) \rceil + 1}^n  n^{k(1 - c/2)} \\
&\leq \sum_{k=4 \cdot \lceil log(n) \rceil + 1}^n n ^ {-k} \text{ for } c = 4\\\
&\leq \sum_{i=1}^n n^{-k} = H_n \leq log(n + 1) \leq 2 \cdot log(n) \text{ for } n \geq 3
\end{align*}
We conclude that \(\E[X] \leq  4 \cdot log(n) + 2 \cdot log(n)\). Equivalently, for \(c' = 6\) we have \(\E[X] \leq c' log(n)\), which is what we intended to show. For \(n \in \{1,2\} \) we can fix the constant to be a very large number. 

\subsection*{b)}
Assuming \(\E[X] = \Theta(log(n))\) we can express the desired probability's negation in a convenient fashion.
\begin{align*}
&\Pr[X \notin  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]] \\
&= \Pr[X < (1 - \alpha(n)) \E[X]\ \wedge  X > (1 + \alpha(n)) \E[X]] \\
&= \Pr[X - \E[X] < - \alpha(n) \E[X]\ \wedge X - \E[X] > \alpha(n) \E[X]] \\
&= \Pr[|X - \E[X]| > \alpha(n)\E[X]] \\
& \leq \Pr[|X - \E[X]| \geq \alpha(n)\E[X]]
\end{align*}
We recognize a form from Lemma 8.2. In order to make use of it, we need to ensure its conditions are satisfied. Let \( \Omega = \Omega_1 \times \dots \times \Omega_N\) with \(N = n\) and each \(\Omega_i\) describing whether all of the \(n-1\) edges of vertex \(i\) are included or not. It follows directly that the effect of each dimension \(c_i\) is at most 1. Let \(\omega \in \Omega\) be a selection of such vertices and \(X(\omega)\) its maximum clique size. Setting \(r = X(\omega)\) trivially fulfills \(r \leq X(\omega)\). Let us define \(J\) to be the subset of \(\{1, \dots, n\}\) that indicates which vertices compose a \(X(\omega)\)-clique - arbitrarily in case of ties. Then, for each \(\omega' \in \Omega\) with \(\forall i \in J, w'_i = w_i\), the size of the maximum clique in \(\omega'\) can only increase, i.e. \(X(\omega') \geq X(\omega) = r\). Moreover:
\begin{align*}
 \sum_{i \in J} c_i^2 \leq \sum_{i \in J} 1 = |J| = X(\omega) = r
 \end{align*}
Thereby the necessary conditions hold true and Lemma 8.2.'s application yields:
\begin{align*}
&\Pr[X \notin  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]] \\
&\leq 4 e^{- \Omega(\frac{(\alpha(n)\E[X])^2}{\E[X] + \alpha(n) \E[X]})} =  4 e^{- \Omega(\frac{\alpha(n)^2\E[X]}{1 + \alpha(n)})}\\
\end{align*}
We define \(\alpha(n) = log(n) ^ {-1/4} \in o(1)\). 
\begin{align*}
\lim_{n \to \infty} \frac{\alpha(n)^2 log(n)}{1 + \alpha(n)} = \lim_{n \to \infty} \frac{log(n)}{\sqrt{log(n)} \cdot 1} = \lim_{n \to \infty} \sqrt{log(n)} \to \infty\\
\end{align*}
It follows directly that \(\Pr[X \notin  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]] \) tends to 0 for large enough \(n\).
\begin{align*}
&\Pr[X \in  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]]  \\
&= 1 - \Pr[X \notin  [(1 - \alpha(n)) \E[X], (1 + \alpha(n)) \E[X]]] \\
& \to 1
\end{align*}

\section*{Exercise 2}
We define \((Y_t)_{t \in \mathbb{N}}\) to be the amount of 'missing mutants', i.e. \(n - |X_t|\). Hence we are looking to bind \(\E[T]\) for \(Y_t\) from above. We define \(u_t\) and \(v_t\) to be the \(u\) and \(v\) of the \(t\)-th round of the evolutionary process. As \(G = K_n\) and \(V_o = n/2\), we know that \(\Pr[u_t \in V_o] = \Pr[u_t \notin V_0] = 1/2\). \\
The process' drawing process weighs vertices from \(V_t\) with \(r\). Given that \(Y_t = y\), there are \( |X_t| = n - y\) such vertices. Hence:
\begin{align*}
	\Pr[v_t \in X_t] = \frac{r(n-y)}{r(n-y) + 1 \cdot y} \text{ and } \Pr[v \notin X_t] = \frac{1 \cdot y}{r(n-y) + 1 \cdot y} 
\end{align*}
As \(u_t\) is drawn after \(v_t\), there are only \(n-1\) vertices to choose from. In particular, we observe that \(y\) is at most \(n/2\) because \(X_t\) will always include \(V_0\). Hence the size of \(X_t \setminus V_0\) is given by \(|X_t| - |V_0| = n - y - n/2 = n/2 - y\).
\begin{align*}
	\Pr[u_t \in X_t \setminus V_0] = \frac{n/2 - y}{n-1} \text{ and } \Pr[u_t \notin X_t] = \frac{y}{n-1}
\end{align*}
Moreover, in order for \(Y_{t+1}\) to decrease over \(Y_t\), \(v_t\) has to be part of \(X_t\) and \(u_t\) be a node not yet contained in \(X_t\). Hence:
\begin{align*}
	\Pr[Y_{t+1} = Y_t - 1] &= \Pr[v_t \in X_t \wedge u_t \notin X_t] \\
	&= \Pr[v_t \in X_t] \Pr[u_t \notin X_t | v_t \in X_t] \\
	&= \frac{r(n-y)}{r(n-y) + y} \frac{y}{n-1}
\end{align*}
On the other hand, for \(Y_{t+1}\) to increase over \(Y_t\),  \(v_t\) has to be excluded from \(X_t\) and \(u_t\) be already contained in \(X_t\). Hence:
\begin{align*}
	\Pr[Y_{t+1} = Y_t + 1] &= \Pr[v_t \notin X_t \wedge u_t \in X_t \setminus V_0] \\
	&= \Pr[v_t \notin X_t] \Pr[u \in X_t \setminus V_0 | v_t \notin X_t] \\
	&= \frac{y}{r(n-y) + y} \frac{n/2 - y}{n-1}
\end{align*}
We will now inspect the drift in order to find a fitting function \(g\) to apply Theorem 9.11. We observe that \(Y_t\) can only remain, increase by one or decrease by one.
\begin{align*}
	\E[Y_{t+1} | Y_t = y] &= y \Pr[Y_{t+1} = Y_t] + (y-1)\Pr[Y_{t+1} = Y_t - 1] + (y+1)\Pr[Y_{t+1} = Y_t + 1] \\
	&=y - \Pr[Y_{t+1} = Y_t - 1] + \Pr[Y_{t+1} = Y_t + 1] \\
	&=y - (\Pr[Y_{t+1} = Y_t - 1] - \Pr[Y_{t+1} = Y_t + 1]) \\
	&= y -\frac{y}{(r(n-y) +y) (n-1)} (r(n-y) - (n/2 - y)) \\
	&=  y - \frac{y (y(1-r) +n (r-1) + n/2)}{(r(n-y) +y) (n-1)} \\
	&=  y - \frac{y ((r-1)(n-y) + n/2)}{(r(n-y) +y) (n-1)} \\
	&\leq y - \frac{y ((r-1)n/2 + n/2)}{(rn +y) (n-1)} \text{ with } (n-y \geq n/2, n-y \leq n) \\
	&= y - \frac{y \cdot r \cdot n/2}{(rn +y) (n-1)}\\
	&\leq y - \frac{y \cdot r}{2(rn +y)} \text { with } (n-1 \leq n)\\
	&\leq y - \frac{y \cdot r}{2(r+1)n} \text { with } (y \leq n) \\
	&\leq \frac{c \cdot y}{n} \text { with } (c \geq \frac{r}{2(r+1)})
\end{align*}

We observe that \( \int_1^{n/2} \frac{c \cdot y}{n} dy =  \left[ c\cdot y log(n)\right]_1^{n/2} = c\cdot n log(n)\) and \(\frac{1}{\frac{c \cdot 1}{n}} = \frac{n}{c}\). Hence, we can apply Theorem 9.12 with \(Y_0 = n/2, h(y) = \frac{c \cdot y}{n}\):
\begin{align*}
	\E[T] \leq \frac{c}{n} + c\cdot n log(n)
\end{align*} 
In conclusion, we have shown that for \(Y_t\) \(E[T] \in \mathcal{O}(nlog(n))\). According to the definition of \(Y_t\), it \(X_t = V\) if \(Y_t = 0\). Hence the bound on the process end also holds for \(X_t\).

\section*{Exercise 3}
	Our goal is to apply Theorem 9.11, followed by Theorem 9.10. In particular, we define Theorem 9.11's \(C := y\), \(X_0 = n\) and 
	\begin{align*}
		g(i) = \begin{cases}
			\frac{\sqrt i (x+y)}{xy}, i > y \\
			0, i \leq y
			\end{cases}
	\end{align*}
	Our main concern is to show that Theorem 9.11's inequality holds. In other words, we want to find a \(c > 0\), s.t. for all \(i \in \mathbb{N}, i>y\):
	\begin{align*}
		&\sum_{m>c} \Pr[X_{t+1} = m | X_t = i] g(m) \leq g(i) - c \\
		&\Leftrightarrow Pr[X_{t+1} = i-y|X_t = i] g(i-y) + Pr[X_{t+1} = min(i+x,n) | X_t = i] g(min(i+x,n)) \leq g(i) -c \\
		&\Leftrightarrow \frac{x}{x+y} g(i-y) + \frac{y}{x+y}g(i+x) \leq g(i) -c \\
		&\Leftrightarrow  \frac{x}{x+y} g(i-y) + \frac{y}{x+y}\frac{\sqrt{i+x}(x + y)}{xy} - \frac{\sqrt i (x+y)}{xy}\leq -c \\
		&\Leftarrow \frac{x}{x+y}\frac{\sqrt{i-y}(x+y)}{xy} + \frac{\sqrt{i+x}}{x} - \frac{\sqrt i}{x} - \frac{\sqrt i}{y }\leq -c
	\end{align*}
	We can replace \(g(i-y)\) by assuming that \(i-y > y\) because it only makes the inequality harder to satisfy. We observe that the initial condition holds true if:
	\begin{align*}
		\frac{\sqrt{i+x} - \sqrt i }{x} - \frac{\sqrt i - \sqrt{i-y}}{y} \leq -c
	\end{align*}
	With \(f(i) = \sqrt i\), we can apply the Mean Value Theorem for both of those terms.  In other words, we know that there exist \(\xi_1 \in ]i, i+x[\) and \(\xi_2 \in ]i-y, i[\) s.t. \(f'(\xi_1) = \frac{\sqrt{i+x} - \sqrt i }{x} \) and \(f'(\xi_2) = \frac{\sqrt i - \sqrt{i-y}}{y}\). Note that our \(g\) is concave and \(\xi_1, \xi_2\) are distinct. Multiplying and dividing our equation by the same term, we obtain that the intial condition holds true if:
	\begin{align*}
		(\xi_1 - \xi_2) \frac{f'(\xi_1) - f'(\xi_2)}{\xi_1 - \xi_2} \leq -c
	\end{align*} 
	As our \(f'(i) = \frac{1}{2\sqrt i}\) is continuous for \(i > k\), we can apply the Mean Value Theorem once again. Hence there is a \(\xi_3 \in ]\xi_2, \xi_1[\) such that \(f''(\xi_3) = \frac{f'(\xi_1) - f'(\xi_2)}{\xi_1 - \xi_2} \), with \(f''(i) = - \frac{1}{4\sqrt{i^3}}\). A look into the intervals from which \(\xi_1\) and \(\xi_2\) originate, we know that \(\xi_1\) is less than \(i+x\) and \(\xi_2\) more than \(i-y\). Therefore \(\xi_1 - \xi_2\) can be bounded from above by \((i+x) - (i - y) = x + y\). Additionally, \(\xi_3 \leq \xi_1 \leq i +x \leq n +x\).  Combining this knowledge we can state that the initial condition is satisfied if:
	\begin{align*}
		&-(x + y)\frac{1}{4\sqrt{(n+x)^3}} \leq -c \\
		&\Leftrightarrow c \leq (x + y)\frac{1}{4\sqrt{(n+x)^3}}
	\end{align*}
	Therefore we can apply Theorem 9.11 and state that:
	\begin{align*}
		\E[T_y] \leq g(n)/y \leq \frac{\sqrt n (x+y)}{xy} \frac{4 \sqrt{(n+x)^3}}{x+y} \leq \sqrt n \sqrt{4(n+x)^3} \leq k \cdot n^2 
	\end{align*}
	for some large enough \(k\), independent of \(n\). In order to apply Theorem 9.10, we define \(p_0\) to be the probability to 'go left', i.e. \(\frac{x}{x+y}\) and \(C = y\). We observe that \(\E[T_y | X_0 =n]\) is awlays finite as there is no way of 'getting stuck' on the right-hand side of \(y\) and the Markov chain itself is finite. Therfore 
	\(\sum_{m > y} \Pr[X_{t+1} = m| X_t = i]\E[T_y|X_0 = m]\) can be bounded by some large \(B\) for all \(i \leq y\). This insight allows us to apply Theorem 9.10, i.e. \(\E[T] = \E[T_y] + \mathcal{O}(1) \leq k \cdot n^2 + \mathcal{O}(1)\).
\section*{Exercise 4}
\subsection*{(a)}
	Given a subgraph \(H\) of \(G\) on \(\nu\) vertices and \(\mu\) balls, we define \(X\) to be the number of edges in \(H\) and \(X_i\) to be the indicator variable telling us whether a particular edge \(i\) is included in \(H\). We know that there are \(2\mu\) edge candidates as each ball can have at most two edges and no edges between bins exist. The probability of an edge candidate being included in \(H\) is the probability of the bin from the edge in \(G\) being part of \(H\). This latter is the amount of bins in \(H\) divided by the amount of bins in \(G\). Hence:
	\begin{align*}
		\E[X] &= \sum_{i=1}^{2\mu} \E[X_i] \text{ (LOE)} \\
		&= \sum_{i=1}^{2\mu} \frac{\nu - \mu}{n} = \frac{2\mu(\nu - \mu)}{n} \\
		&\leq \frac{2 \nu^2}{n}
	\end{align*}
	We will now apply a Union Bound to make use of our bound for \(\E[X]\). Additionally, we note that there are at most \({n \choose \nu}{\nu \choose \mu}\) ways of constructing subgraphs \(H\) from \(G\) s.t. \(H\) has \(\nu\) vertices and \(\mu\) balls. 
	\begin{align*}
		& \Pr[\text{some subgraph has 3 more edges than vertices}] \\
		&= \Pr[ \bigcup_{H \subseteq G} H \text{ has 3 more edges than vertices}] \\
		&\leq \sum_{H \subseteq G}  \Pr[H \text { has at least }  \nu + 3 \text{ edges} | \text{H has } \nu \text{ vertices}]  \text{ (U.B.)} \\
		&\leq \sum_{\nu = 1}^{1000 log(log(n))} \sum_{H \subseteq G}  \Pr[H \text { has at least } \nu + 3 \text{ edges} | \text{H has } \nu \text{ vertices}]\\
		&\leq \sum_{\nu = 1}^{1000 log(log(n))} \sum_{\mu = 1}^{\nu} \sum_{H \subseteq G}  \Pr[H \text { has at least } \nu + 3 \text{ edges} | \text{H has } \nu \text{ vertices and } \mu \text{ balls}] \\
		&\leq \sum_{\nu = 1}^{1000 log(log(n))} \sum_{\mu = 1}^{\nu} {n \choose \nu} {\nu \choose \mu} \Pr[\text {subgraph has at least } \nu + 3 \text{ edges} |  \text{has } \nu \text{ vertices and } \mu \text{ balls}]
	\end{align*}
	We can now use Markov's inequality saying that for \(X \geq 0, t >0\) \(\Pr[X \geq t] \leq \E[X] / t\). In this particular instance we get that:
	 \begin{align*}
	 	\Pr[\text{subgraph has }  \nu \text{ vertices, } \nu \text{ balls  and at least } \nu + 3 \text{ edges}] &\leq \frac{\E[X]}{\nu +3} \\
	 	&\leq \frac{2 \nu ^2}{n (\nu +3)} \leq \frac{2\nu}{n}
	\end{align*}
	Including this bound in our essential probability, we obtain:
	\begin{align*}
		& \Pr[\text{some subgraph has 3 more edges than vertices}] \\
		&\leq \sum_{\nu = 1}^{1000 log(log(n))} \sum_{\mu = 1}^{\nu} {n \choose \nu} {\nu \choose \mu} \frac{2\nu}{n} \\
		&\leq \sum_{\nu = 1}^{1000 log(log(n))} \sum_{\mu = 1}^{\nu} n^\nu \nu^\mu \frac{2\nu}{n} \\
		&\leq \sum_{\nu = 1}^{1000 log(log(n))} \nu n^\nu \nu^\nu \frac{2\nu}{n} \\
		&\leq \frac{1}{n} \nu_{max} \nu_{max}  n^{\nu_{max}}  \nu_{max} ^{\nu_{max}}  2 \nu_{max} \text{ with } \nu_{max} = 1000 log(log(n)) \\
		&\leq \frac{1}{n} \cdot \mathcal{O} (log(n)^{log(n)}) \\
		& \to 0 \text{ for } n \to \infty
	\end{align*}
\subsection*{(b)}
\subsection*{(c)}
\begin{enumerate}[(i)]
	\item
	By definition of the construction of the tree, we know that we have exactly one leaf, a ball with label 6, per bin and one bin per leaf. In other words \(t(h) = \nu(h)\).  \\
	The presence of the ball of height \(h\), connected to bin \(u\), implies the presence of a leaf associated with \(u\). Moreover, the subtrees starting in balls labeled 7 to \(h-1\) connected to \(u\) will contain leaves. Hence: \(t(h) = 1 + \sum_{i=7}^{h-1} t(i) \).
	\begin{claim} 
		\(\forall h \in \mathbb{N}, h \geq 7: t(h) = 2^{h-7}\)
	\end{claim}
	\begin{proof} 
		We observe that for \(h = 7\), there is one leaf, i.e. \(t(7) = 1 = 2 ^{7-7}.\) We will assume our hypothesis for some \(h \geq 7\) and will show it holds for \(h+1\) as well.
		\begin{align*}
			t(h+1) &= 1 + \sum_{i=7}^{h} t(i) \\
			&= 1 + \sum_{i=7}^{h}2^{i-7} \text{ (induction hypothesis)} \\
			&= 1 + (2^{h+1-7} - 1) = 2^{h+1-7}
		\end{align*}
	\end{proof}
	\item
	Knowing that the witness graph is a tree, we know that we can add up, for a bin, the leaf, the ball of greatest height \(h\) and all balls in subtrees rooted in \(7\) to \(h-1\) connected to the bin, without double-counting or missing a ball. Hence \(\mu(h) = 2 + \sum_{i=7}^{h-1} \mu(i)\). 
	\begin{claim}
		\(\forall h \in \mathbb{N}, h \geq 7: \mu(h) = 2^{h-6}\)
	\end{claim}
	\begin{proof} 
		We observe that for \(h = 7\), there is one leaf as well as the ball labeled 7, i.e. \(\mu(7) = 2 = 2 ^{7-6}.\) We will assume our hypothesis for some \(h \geq 7\) and will show it holds for \(h+1\) as well.
		\begin{align*}
			\mu(h+1) &= 2 + \sum_{i=7}^{h} \mu(i) \\
			&= 2 + \sum_{i=7}^{h}2^{i-6} \text{ (induction hypothesis)} \\
			&= 2 + 2(2^{h+1-7} - 1) = 2^{h+1-6}
		\end{align*}
	\end{proof}


\item 
See (i).
\end{enumerate}
\subsection*{(d)}
\end{document}
