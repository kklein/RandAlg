
\documentclass[a4paper,german]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}

\newcommand\loe{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily LOE}}}{=}}}
\newcommand\ub{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily UB}}}{\leq}}}
\newcommand\gequb{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily UB}}}{\geq}}}

%This is how you can create a "claim"-environment (or a lemma/Theorem/definition etc environment)
\newtheorem{claim}{Claim}

%This is how you can create a command of your own, e.g. to simplify the usage signs you often use.
\newcommand{\E}{\mathbb{E}}

%Titlepage:
\title{Randomized Algorithms and Probabilistic Methods: Graded Homework 1}
\author{ Kevin Klein\\Collaborators: Ramon Braunwarth, Fabian Gerhardt, Emanuel Malvetti}
\date{November 14, 2017}


\begin{document}
\maketitle

\section*{Exercise 1}
We define the random variable \(X\) to be the size of a maximal neighbor-matching and \(X_i\) to be the number of balls in bin \(i\). We know that if each bin has at least \(k\) balls, we can for sure find a neighbor-matching of size \(k \cdot \frac{n}{2}\). In other words, \(k\) is the mimimum of the loads of the bins, i.e. \(\min_i X_i\). We observe that in section 6.1. of the lecture's script, it has been shown that those \(X_i\) are negatively associated with expected value \(\frac{m}{n}\). This permits us to apply the Chernoff bound 5.4 (ii).
\begin{align*}
\Pr[X < (1-\epsilon)\frac{m}{2}] &\leq \Pr[\min_i X_i \cdot \frac{n}{2} < (1 - \epsilon) \frac{m}{2}] \\
& \leq \Pr[\bigcup_i X_i \cdot \frac{n}{2} < (1 - \epsilon) \frac{m}{2}] \\
& \ub \sum_{i=1}^n  \Pr[X_i \cdot \frac{n}{2} < (1 - \epsilon) \frac{m}{2}] \\
& \leq n  \Pr[X_1 \cdot \frac{n}{2} < (1 - \epsilon) \frac{m}{2}] \\
& \leq n  \Pr[X_1 \cdot < (1 - \epsilon) \frac{m}{n}] \\
& \leq n  \Pr[X_1 \cdot \leq (1 - \epsilon) \frac{m}{n}] \\
& \leq n e^{-\frac{m}{n} \frac{\epsilon^2}{2}} \text{     (via Chernoff bound)} \\
\end{align*}
We observe that the latter value tends to zero for large enough n and \(m \gg nlog(n)\). Hence, for \(m \gg nlog(n)\), \( \Pr[X \geq (1-\epsilon)\frac{m}{2}] \) hols whp.
\section*{Exercise 2}
First, we want to look into a single entry of \((Aq)\), say \((Aq)_1 = a_1\). We know that \(q \in_{u.a.r.} \{-1,1\}\) and define \(q' = \frac{q+1}{2}\). Intuitively, \(q'\) 'counts' how many indeces of \(q\) equal 1. We observe that \(q = 2q' - 1\).  Moreover we say \(m\) is the number of indeces in \(a_1\) that equal 1, i.e. \(m = \sum_{i=1}^n \mathbb{I}[a_{1i} = 1] \).
\begin{align*}
\Pr[|a_1q| \geq \sqrt {(6+\epsilon)nln(n)} ] &= \Pr[|2a_1q' - 2a_11| \geq \sqrt {(6+\epsilon)nln(n)} ] \\
 &= \Pr[|2a_1q' - m| \geq \sqrt {(6+\epsilon)nln(n)} ] \\
  &= \Pr[|a_1q' - m/2| \geq \frac{\sqrt {(6+\epsilon)nln(n)}}{2} ]
\end{align*}
Note that \(q_i'\) is a Bernoulli random variable with \(p = 1/2\), independent from all other \(q_j', i \neq j\), this will come in handy in order to use Chernoff bounds. Moreover we notice that \(\E[a_1q'] = 
a_1 \E[q'] = a_1 \cdot 1/2 = m/2 \).  Applying Corollary 5.4 (iii), we obtain:
\begin{align*}
 \Pr[|a_1q' - m/2| \geq \frac{\sqrt {(6+\epsilon)nln(n)}}{2} ] &\leq 2 e^{-(\frac{\sqrt {(6+\epsilon)nln(n)}}{2})^2 \frac{1}{3 \mu }} \\
 & \leq 2 n^{-\frac{(6+\epsilon)n}{12 m /2}} =  2 n^{-\frac{6+\epsilon}{6} \frac{n}{m}} \\
 & \leq 2 n^{-\frac{6+\epsilon}{6}} & \text{(via } m < n \text{)}
\end{align*}
It remains to show that this application was legitimate by proving that the formula's \(\delta = \frac{\sqrt {(6+\epsilon)nln(n)}}{m/2}  \in (0,1] \) for large enough \(n\). 
\begin{claim} \(\delta\) is smaller than 1 for large enough \(n\). \end{claim}
\begin{proof}
\begin{align*}
\lim_{n \to \infty} \delta &= \lim_{n \to \infty} \frac{\sqrt {nln(n)}}{m}  \leq \lim_{n \to \infty} \frac{\sqrt {nln(n)}}{n}  \\
& \leq \lim_{n \to \infty} \sqrt \frac{ {ln(n)}}{n}  \to 0
\end{align*}
\end{proof}
Hence \(\delta\) is smaller than 1 for large enough \(n\) and trivially positive, which satisfies the Chernoff bound's condition.

Our goal now is to use the bound on the probability of the first entry of \( (Aq) \) for all entries via a union bound.
\begin{align*}
\Pr[ \lVert Aq \lVert_{\infty} < \sqrt {(6+\epsilon)nln(n)}] &= \Pr[ \forall i  |(Aq)_i|  < \sqrt {(6+\epsilon)nln(n)}] \\
&= 1 - \Pr[ \exists i  |(Aq)_i|  \geq \sqrt {(6+\epsilon)nln(n)}] \\
&\gequb 1 - \sum_{i=1}^n \Pr[|(Aq)_i|  \geq \sqrt {(6+\epsilon)nln(n)}] \\
&\geq 1 - n \Pr[|(Aq)_1|  \geq \sqrt {(6+\epsilon)nln(n)}] \text{     (independence of rows)}\\
&\geq 1 - n 2 n^{-\frac{6+\epsilon}{6}}  \xrightarrow{\: n \to \infty \: } 1
\end{align*}
\section*{Exercise 3}
We already know about the relation between \(\gamma(G)\) and \( \gamma(H)\). In addition we obserbe a useful inequality between the expected values of \(\gamma(G)\) and \( \gamma(H)\). <<<
\begin{align*}
\E[\gamma(H)] & = \frac{1}{2} (\E[\gamma(H)] + \E[\gamma(H')]) \\
&\loe \E[\gamma(H) + \gamma(H')] \\
&\geq \frac{\E[\gamma(G)]}{2} \\
&\geq \frac{\gamma(G)}{2}
\end{align*}
Now we aim to structure the problem as to easily apply Azuma's inequality with vertex-exposure. \\
Let \(W\) be the set of nodes incident to the edges of \(E'\), i.e. \(|W| \leq 2\gamma(G)\). For each node in \(w_i W\), we define \(E_i = \{\{(w_i, v\}, \{(w_i, v\} \in E, v \notin W\}\). For each edge \(e'_i \in E'\), we define \(E_{|W| + i} = \{e'_i\}\). The \(E_1, \dots, E_{|W|}, E_{|W| + 1}, \cdots, E_{|W| + \gamma(G)}\) are a partition of \(E\). We associate a probability space \(\Omega_i\) with \(E_i\).
We now need to calculate the effect of each of these probability spaces. In order to do so, we look into how we can create an edge-dominating set for H, if given edges are removed. If \(E_i\) contains an edge from \(E'\), taking or not taking it changes the size of the edge-dominating set of 1, as each edge from the dominating edge can be replaced by one egde per vertex. If \(E_i\) contains egdes to \(W\), a single edge suffices to cover all edges. 
Hence we can bound \(c_i\) easily by 2.
Summing up, we get
\begin{align*}
\Pr[\gamma(H) \leq 40000)] &\leq \Pr[\gamma(H) \leq \E[\gamma(H) - 10000] \\
&\leq e^{-\frac{100000000}{2 \sum_{i=1}^{|W| + \gamma(G)} c_i^2}} \\
&\leq e^{-\frac{100000000}{2 \cdot 300000 \cdot 4}} \\
& \leq e^{-42} \\
& < \frac{1}{1000}
\end{align*}
\section*{Exercise 4}
\subsection*{Exercise 4.1}
First, we'll bound the probability of a given subset of size \(n/2\) not containing a triangle. In such a subset, there are \( {n/2 \choose 3}\) many possible triangles, over which we assume an arbitrary order. We define 
\begin{align*} 
T_i &= \mathbb{I} [\text{triangle } i \text{ is present in subset } 1] = p^3 \\
T &= \sum_{i \in {n/2 \choose 3}} T_i \\
\lambda &= \E[T] \loe \sum_{i \in {n/2 \choose 3}} \E[T_i] = {n/2 \choose 3} p^3
\end{align*}
In order to calculate \(\Delta\), we notice that \(I_i = \{e_{i1}, e_{i2}, e_{i3} \}\), where \(e_{ik}\) are the edges constructing the triangle \(i\). We proceed by formulating a case distinction on the number of shared edges between two triangles \(i\) and \(j\).
\begin{enumerate}
\item If  they share no edges, \(I_i \cap I_j = \emptyset \).
\item If they share 1 edge, the probability of both being present is equal to the probability of those 5 eges being selected, i.e. \(\Pr[T_i \wedge T_j] = p^5\).
\item If they share 2 edges, the third edge needs to be the same for both triangles as there is at most a single edge connecting two nodes. Hence \(i = j\).
\item If they share 3 edges, \(i = j\) holds trivially. 
\end{enumerate}
We can conclude that the only relevant case is the second. It remains to inspect how frequently this case can occur. In a given subset, there are \( {n/2 \choose 3} \) possible triangles, of which each can share each of its 3 edges. When sharing an edge, the second triangle can be 'completed' by \(n/2 -3\) remaining nodes. Summing up, we have:

\begin{align*}
\Delta = {n/2 \choose 3}3(n/2 -3) p^5
\end{align*}
Janson's equality tells us:
\begin{align*}
\Pr[T = 0] = e^{- min\{\lambda, \lambda^2 / \Delta\} /4} 
\end{align*}
Let us determine said minimum.
\begin{align*}
\frac{\lambda^2}{\Delta} &= \frac{({n/2 \choose 3} p^3)^2}{ {n/2 \choose 3}3(n/2 -3) p^5} = \frac{{n/2 \choose 3}p}{3(n/2 -3)} \\
&= \frac{n^3p}{n} \text{     (for large enough \(n\))} \\
&= n^2p \\
&> n^2 n^{-2/3} \text{     (via } p\gg n^{-2/3} \text{)} \\
&> n = n^3 n^{-2} \\
&> n^3 p^3 = \lambda  \text{     (via } p\gg n^{-2/3} \text{)}
\end{align*}
Thus, as \( \lambda < \frac{\lambda^2}{\Delta}\) for given \(p\), it follows:
\begin{align*}
\Pr[T = 0] &= e^{-\lambda/4} = e^{-\frac{n^3p^3}{4}} \\
&= e^{-\omega(n)}    \text{     (via } p\gg n^{-2/3} \text{)}
\end{align*}

We will now use this knowledge about the probability of a given subset containing no triangle to estimate the probability of each subset containing a triangle. We observe that there are \( {n \choose n/2}\) ways to partition the nodes into subsets of size \(n/2\). 
\begin{align*}
\Pr[\text{each subset contains a triangle}] &= \Pr[ \bigwedge_i \text{subset } i \text{ contains a triangle}] \\
&= 1 - \Pr[\bigvee_i \text{subset } i \text{ contains no triangle}]
\end{align*}
We look into the latter probability.
\begin{align*}
&\Pr[\bigvee_i \text{subset } i \text{ contains no triangle}]  \\
&\ub \sum_i \Pr[\text{subset } i \text{ contains no triangle}] \\
&\leq  {n \choose n/2} \Pr[\text{subset } 1 \text{ contains no triangle}] \text{   (all subsets symmetric)} \\
&\leq \frac{2^n}{\sqrt n}  \Pr[\text{subset } 1 \text{ contains no triangle}] \text{   (Stirling approximation)} \\
&\leq \frac{2^n}{\sqrt n}  \Pr[T = 0]\\
&\leq \frac{2^n}{\sqrt n} e^{-\omega(n)} = \frac{e^{ln(2)n - \omega(n)} }{\sqrt n} \\
&\leq \frac{1}{\sqrt n \cdot e^{\omega(n) - \mathcal{O}(n)}} \xrightarrow{\: n \to \infty \: } 0
\end{align*}

It follows directly that:
\begin{align*}
\Pr[\text{each subset contains a triangle}] \xrightarrow{\: n \to \infty \: }  1 - 0 = 1
\end{align*}
\subsection*{Exercise 4.2}
We claim that \( p = n^{-2/3}\) is a weak threshold and thereby want to prove that
\[\lim_{n \to \infty} Pr [G_{n,p} \text{is almost triangular}] = 
 \begin{cases}
 	0, & p \ll n^{-2/3} \\
	1, & p \gg n^{-2/3} \\
    \end{cases}\]
We aim to show the first case first and therefore assume \( p \ll n^{-2/3}\). Let's say that \(X\) represents the number of triangles in a random graph \(G_{n,p}\), then we know that \( \E [X] = {n \choose n/3} p^3\). The number of vertex-disjoint triangles in \(G_{n,p}\) has to be smaller or equal the number of triangles in a graph. Hence, with \(X' = \# \)vertex-disjoint triangles in \(G_{n,p}\), we can say \(\E[X'] \leq \E[X]\). We can apply Azuma's inequality to \(X'\) with node-exposure: as the triangles are vertex-disjoing, no more than 1 triangle can be affected by one node. In other words \(\forall i \in \{1, \dots, n\}\ c_i \leq 1 \). We choose some sublinear \(t\), say \( t = n^{3/4}\). We obtain:
\begin{align*}
\Pr[X' \geq \E [X'] + n^{3/4}] \leq e^{-\frac{n^{3/2}}{2n1}} \leq e^{\frac{-\sqrt n}{2}}  \xrightarrow{\: n \to \infty \: }  0
\end{align*}
As \(\E[X']\) is a little tricky to compute, we will only compare it to \(\E[X]\) and make use of  \(p \ll n^{-2/3}\).
\begin{align*}
\E[X'] \leq \E[X] = {n \choose 3}p^3 \leq n^3p^3  \ll n^3n^{-2} = o(n)
\end{align*}
As \(n^{3/4} = o(n)\), we can say that for large enough \(n\):
\begin{align*}
\Pr[X' \geq \frac{(1-\epsilon)n}{3}] = \Pr[X' \geq \Theta(n)] \leq \Pr[X' \geq o(n) + o(n)] \leq  \Pr[X' \geq \E [X'] + n^{3/4}]  \xrightarrow{\: n \to \infty \: }  0
\end{align*}
which is what we intended to show for the first case. \\
Let us now look into the second case and assume \( p \gg n^{-2/3}\). For this purpose we define the random variable \(T\) to be the number of triangles in a subset of \(\epsilon n\) nodes of \(G_{n,p}\). Such a subset is drawn u.a.r. from all possible subsets of that size. The expected value of said random variable is \(\lambda = \sum_{\epsilon n \choose  3} T_i = {\epsilon n \choose  3} p^3 \), where \(T_i\) indicates the 'presence' of a particular triangle in the fixed subset. In order to apply Janson's inequality, we observe that \(\Delta = {\epsilon n \choose 3} 3 (\epsilon n -3) p^5 \) - analogously to Exercise 4.1. As \(\epsilon\) is a constant independent of \(n\), we can argue that, also analogously to Exercise 4.1, \(\lambda < \frac{\lambda^2}{\Delta} \) for large enough \(n\), because we have assumed that \( p \gg n^{-2/3}\). Therefore Janson's inequality gives us:
\[ \Pr [T = 0] \leq e^{-\frac{{\epsilon n \choose 3}p^3}{4}} \leq e ^{-\omega(n)}\]
Hence we know that in a subset of \(\epsilon n \) nodes, the presence of a triangle is very likely. At the same time, we observe that we can use \( {\epsilon n \choose 3} \leq 2^{\epsilon n} \leq 2^n \) in order to arrive at:
\begin{align*}
\Pr[\text{each subset of size } \epsilon n \text{ contains a triangle} ] \geq 1 - 2^n e^{- \omega(n)} \xrightarrow{\: n \to \infty \: }  1
\end{align*} 
just as we have shown in Exercise 4.1.

\begin{claim} If each subset of size \(\epsilon n\) contains a triangle, then there are at least \(\frac{(1 - \epsilon)n}{3}\) many vertex-disjoint triangles. 
\end{claim}
\begin{proof}
Assume that each susbet of size \(\epsilon n\) contains a triangle and that there are \(t < \frac{(1 - \epsilon)n}{3}\) vertex-disjoint triangles. We define \(V_t\) to be the set of nodes that belong to the vertex-disjoint triangles in the maximal constellation. It follows immediately:
\begin{align*}
|\overline{V_t}|= n - |{V_t}| > n - (1 - \epsilon) n = \epsilon n
\end{align*}
Therefore \(\overline{V_t}\) is sufficiently large to contain a triangle, according to our assumption. This triangle has no nodes in with \(V_t\) and therefore is another vertex-disjoint triangle, not accounted for in the assumed maximum. This concludes the contradiction and proves the claim.
\end{proof}

\begin{align*}
&\Pr[G_n,p \text{ has } \frac{(1 - \epsilon)n}{3} \text {vertex-disjoint triangles}]  \\
& \geq \Pr[\text{each subset of size } \epsilon n \text{ contains a triangle}] \\
& \geq 1 - 2^n e^{- \omega(n)} \xrightarrow{\: n \to \infty \: }  1
\end{align*}
Thus, we have shown both cases that were to be shown.
\end{document}
