
\documentclass[a4paper,german]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}


%This is how you can create a "claim"-environment (or a lemma/Theorem/definition etc environment)
\newtheorem{claim}{Claim}

%This is how you can create a command of your own, e.g. to simplify the usage signs you often use.
\newcommand{\E}{\mathbb{E}}

%Titlepage:
\title{Randomized Algorithms and Probabilistic Methods: Graded Homework 1}
\author{ Kevin Klein, list of collaborators}
\date{October 23rd 2017}


\begin{document}
\maketitle

\section*{Exercise 1}
Let's define \(C = \bigcup_{v} C_v\). If we can find a partition \(C = C_A \cup C_B\) such that \(\forall v \in A: C_v \cap C_A \neq \emptyset \) and \(\forall v \in B: C_v \cap C_B \neq \emptyset \), we have found a trivial admissable coloring. The admissable coloring could be constructed by coloring each node with an arbitrary color from said intersection.
We now want to show that, given the minimal length of \(C_v\), a random partitioning can lead to an admissable coloring. \\

We assign each \(c \in C\) to either \(C_A\) or \(C_B\) u.a.r. This is a partition as every color is assigned to exactly one set. 
When inspecting node \(v \in A\), we define \emph{success} to be the existence of \(c \in C_V \cap C_A \neq \emptyset\). The case for which \(v \in B\) follows by symmetry. Success for all nodes implies that each node can chose a color which is not chosen in the class containing its neighboring nodes, i.e. there is an admissable coloring. 

\begin{align*} X_i &= \text{\emph{success} for node } i  \\
&= \mathbbm{1}[C_i \cap C_{class(i)} \neq \emptyset] \\
\Pr[X_i] &= 1 - \Pr[\text{all } c \in C_i \text{ have been mapped to } C - C_{class(i)}] \\
 &= 1 - \frac{1}{{2}^{|C_i|}} \\
 X &= \text{\#\emph{successes} among all } 2n \text{ nodes}\\
 &= \sum_{i=1}^{2n} X_i \\
 \mathbb{E}[X] &= \mathbb{E}[\sum_{i=1}^{2n} X_i ]\\
 &= \sum_{i=1}^{2n} \mathbb{E}[X_i ] & \text{(by linearity of expectation)} \\
 &=  \sum_{i=1}^{2n} \Pr[X_i ] \\
&=  \sum_{i=1}^{2n} (1 - \frac{1}{{2}^{|C_i|}}) 
\end{align*}

We can now make use of our knowledge about the size of each \(C_v\).

\begin{align*}
|C_v| > log_2n + 1 &\Rightarrow \frac{1}{{2}^{|C_v|}} < \frac{1}{{2}^{log_2n + 1}} = \frac{1}{2n}\\
&\Rightarrow 1 - \frac{1}{{2}^{|C_v|}} > 1 - \frac{1}{2n} 
\end{align*}

We can now use this this bound to reformulate the expectation.

\begin{align*} \mathbb{E}[X] &> \sum_{i=1}^{2n}  (1 - \frac{1}{2n})  \\
&> 2n(1 - \frac{1}{2n} ) \\
&> 2n - 1 \\
\end{align*}
By the probabilistic method, there has to be at least one realistion of \(X\) attaining value at least \(2n\).  The latter implies \emph{success} for each node, which, as we have argues, implies the existence of an admissable coloring.

\section*{Exercise 2}

Firstly, we observe that there are \( {n} \choose {3} \) possible edges in total, \( {n-1} \choose {2} \)  possible edges containing a node \(i\) and \( {n-2} \choose {1} \)  possible edges containing nodes \(i\) and \(j\). 
\begin{align*}
X_i &= \text{node } i \text{ is isolated} \\
\Pr[X_i] &= \Pr[\text{all possible edges containing } i \text{ have not been added}] \\ 
&= (1-p)^ {{n-1} \choose {2}} \\
&= \mathbb{E} [X_i] \\
X &= \# \text{isolated nodes} \\
\mathbb{E}[X] &= \mathbb{E}[\sum_{i=1}^n X_i] \\
&= \sum_{i=1}^n  \mathbb{E}[X_i]  & \text{(by linearity of expectation)} \\
&= n \mathbb{E}[X_i]
\end{align*}

\begin{enumerate}

\item Say \(p = (1+\epsilon)\frac{2ln(n)}{n^2} \)
\begin{align*}
\lim_{n \to \infty} \mathbb{E}[X] &= \lim_{n \to \infty} n (1-p)^ {{n-1} \choose {2}} \\
&\leq \lim_{n \to \infty} n e^{-p{{n-1} \choose {2}}} \\
&\leq \lim_{n \to \infty} n e^{\frac{-2(1+ \epsilon) ln(n)(n-1)(n-2)}{2n^2}} \\
&\leq \lim_{n \to \infty} n n^{\frac{-(1+ \epsilon) (n-1)(n-2)}{n^2}} \\
&\leq \lim_{n \to \infty} n n^{-(1+ \epsilon)} \to 0
\end{align*}
As \(\mathbb{E}[X]\) cannot be negative, we conclude that:
$$ \lim_{n \to \infty} \mathbb{E}[X] \to 0$$
As the \(X\) is guaranteed to be non-negative, we can apply the first moment method:
$$ \Pr[X = 0] = 1 - o(1)$$

\item Say \(p = (1- \epsilon)\frac{2ln(n)}{n^2} \) \\

Firstly, let's demonstrate a useful inequality.
\begin{align*}
& n^4 \leq e^{n^2} \text{ which holds trivially for large \(n\)} \\
&\Rightarrow 4ln(n) \leq n^2\\
& \Rightarrow \frac{2ln(n)}{n^2} \leq \frac{1}{2}\\
& \Rightarrow (1-\epsilon) \frac{2ln(n)}{n^2} \leq \frac{1}{2}\\
\end{align*}
text text
\begin{align*}
\lim_{n \to \infty} \mathbb{E}[X] &= \lim_{n \to \infty} n (1-p)^ {{n-1} \choose {2}} \\
&\geq \lim_{n \to \infty} n e^{-p(1 - p) \frac{(n-1)(n-2)}{2}}   & \text{(via Fact 2, as } 0 \leq p \leq \frac{1}{2} \text{)}\\
&\geq  \lim_{n \to \infty} n e^{ - (1 - \epsilon) \frac{2 ln(n)}{n^2} (1-(1-\epsilon)\frac{2ln(n)}{n^2}) \frac{(n-1)(n-2)}{2}}\\
&\geq \lim_{n \to \infty} n n^{- (1-\epsilon) (1 - (1- \epsilon)\frac{2ln(n)}{n^2})} 
\end{align*}

We observe that 
\( \lim_{n \to \infty} \frac{2ln(n)}{n^2} \to 0 \) and \( \lim_{n \to \infty} 1 - (1-\epsilon)\frac{2ln(n)}{n^2} \to 1 \).
Hence:

\begin{align*}
\lim_{n \to \infty} \mathbb{E}[X] &\geq \lim_{n \to \infty} n n^{-(1 - \epsilon)} \to \infty\\
\end{align*}

In order to compute the variance, let's first have a look at the provavility of two distinct nodes \(i\) and \(j\) being isolated at the same time. All edges that would either involve i and j must not be added. To avoid double-counting, we shall not forget to take the edges containing both i and j into consideration.

\begin{align}
\Pr[i \text{ and } j \text{ are isolated}] &= (1-p)^{ {{n-1} \choose {2}} + {{n-1} \choose {2}} - {{n-2} \choose {1}} } \nonumber\\ 
&=  (1-p)^{(n-1)(n-2) - (n-2) } \nonumber \\
&=  (1-p)^{(n-2)(n-1 -1 ) } \nonumber \\
&=  (1-p)^{(n-2)(n-2) }  \label{eq1} 
\end{align}

Furthermore, we will introduce a hepful inequality.

\begin{align}
& \forall n > 0: \frac{n-1}{2} < n - \frac{2ln(n)}{n} \nonumber \\
& \Rightarrow \frac{n-1}{2} < n - (1-\epsilon) \frac{2ln(n)}{n} \nonumber \\
& \Rightarrow \frac{n-1}{2} < n(1 - (1-\epsilon) \frac{2ln(n)}{n^2}) \nonumber \\
& \Rightarrow \frac{n-1}{2}  < n(1-p) \nonumber \\
& \Rightarrow \frac{n(n-1)}{2}  < n^2(1-p) \nonumber \\
& \Rightarrow {{n} \choose {2}} (1-p)^{n-2} < n^2(1-p)^{n-1} \nonumber \\
& \Rightarrow {{n} \choose {2}} (1-p)^{(n-2)(n-2)} < n^2(1-p)^{(n-1)(n-2)} \nonumber \\
& \Rightarrow {{n} \choose {2}} (1-p)^{(n-2)(n-2)} < \mathbb{E}[X]^2 \label{eq2}
%n > 3 &\Rightarrow \frac{(n-1)(n-2)}{2} < (n-2)(n-2)\\
%&\Rightarrow (1-p)^{\frac{(n-1)(n-2)}{2}} > (1-p)^{(n-2)(n-2)}
\end{align}

Combining this knowledge allows us to bound the variance of \(X\).

\begin{align*}
\text{Var}[X] &= \mathbb{E}[X^2] - (\mathbb{E}[X])^2\\
&= \mathbb{E}[\sum_{i=1}^n \sum_{j = 1}^n X_i X_j] - (\mathbb{E}[X])^2\\
&= \sum_{i=1}^n \sum_{j = 1}^n \mathbb{E}[X_i X_j]  - (\mathbb{E}[X])^2 & \text{(by linearity of expectation)}\\
&= \sum_{i=1}^n \mathbb{E}[X_i] + \sum_{(i,j) \in {{V} \choose{2}}} \mathbb{E}[X_i X_j]  - (\mathbb{E}[X])^2 \\
&= \mathbb{E}[X] +   \sum_{(i,j) \in {{V} \choose{2}}} \Pr[i \text{ and } j \text{ are isolated}] - (\mathbb{E}[X])^2 \\
&= \mathbb{E}[X] + {{n} \choose {2}}   (1-p)^{(n-2)(n-2) } - (\mathbb{E}[X])^2 &  \text{(via \ref{eq1}) } \\
&< \mathbb{E}[X] +  (\mathbb{E}[X])^2   - (\mathbb{E}[X])^2 & \text{(via \ref{eq2})} \\
&< \mathbb{E}[X]
\end{align*}

As we know that for relevant \(p\), \(\lim_{n \to \infty} \mathbb{E}[X] \to \infty\), it holds that \(\mathbb{E}[X] \in o (\mathbb{E}[X]^2) \). Thus, Var\([X] \in o (\mathbb{E}[X]^2) \). The second moment method implies that:
$$ \Pr[X=0] = o(1) $$.

\end{enumerate}

Summing up, we have:
\[
    \Pr[X=0] = 
\begin{cases}
   1 - o(1) & p \geq (1+\epsilon)\frac{2ln(n)}{n^2}\\
   o(1)             & p \leq (1- \epsilon)\frac{2ln(n)}{n^2}
\end{cases}
\]

which corresponds to the definition of a sharp threshold.

\section*{Exercise 3}
\section*{Exercise 4}
\begin{enumerate}
\item
Having \(i\) elements already in her list per position, the probability of Alice's next guess being a zero-guess is \((\frac{n-i-1}{n-i})^n\). We now denote \(X_i \) to be the number of guesses required to obtain the \(i\)th zero-guess. Observe that this expectation is the inverse of the the latter probability. \(X\) will be refered to as the number of required guesses to arrive at the last, i.e. \(n-1\)th zero-guess.
\begin{align*}
X &= \sum_{i=1}^{n-1} X_i \\
\mathbb{E}[X] &= \mathbb{E}[\sum_{i=1}^{n-1} X_i ]\\
&= \sum_{i=1}^{n-1} \mathbb{E}[X_i] & \text{(by linearity of expectation)} \\
&= \sum_{i=0}^{n-2}\frac{1}{ (\frac{n-i-1}{n-i})^n}\\
&\geq \sum_{i=n-2}^{n-2} (\frac{n-i}{n-i-1})^n = (\frac{2}{1})^n
\end{align*}
Hence we have shown that \( \mathbb{E}[X] \in \Omega(2^n) \).
\item
\item
\item
\end{enumerate}
\end{document}
